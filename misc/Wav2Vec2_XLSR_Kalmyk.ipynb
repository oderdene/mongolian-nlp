{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wav2Vec2-XLSR-Kalmyk.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNpFf5X1-R4b"
      },
      "source": [
        "# Kalmyk Speech Recognition with Wav2Vec2-XLSR\n",
        "\n",
        "This notebook uses HuggingFace finetuned [Wav2Vec2 XLSR](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/) model on a Kalmyk Bible dataset.\n",
        "\n",
        "Click on \"Runtime -> Run All\" and after that, you can either record your voice or upload a WAV file to transcribe.\n",
        "\n",
        "For other deep-learning Colab notebooks, visit [tugstugi/dl-colab-notebooks](https://github.com/tugstugi/dl-colab-notebooks)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "antcD_Q3_d3X"
      },
      "source": [
        "## Installation\n",
        "* install HuggingFace\n",
        "* download a trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnWntVYj6E63",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from os.path import exists, join, basename, splitext\n",
        "\n",
        "# download model and install dependencies\n",
        "if not exists('wav2vec2-large-xlsr-53-kalmyk'):\n",
        "  !touch wav2vec2-large-xlsr-53-kalmyk\n",
        "  !pip install -q git+https://github.com/huggingface/transformers.git\n",
        "  !pip install -q gdown librosa\n",
        "  !pip install -q https://github.com/tugstugi/dl-colab-notebooks/archive/colab_utils.zip\n",
        "\n",
        "  from transformers import Wav2Vec2Processor\n",
        "  from transformers import Wav2Vec2ForCTC\n",
        "  # load the model\n",
        "  model = Wav2Vec2ForCTC.from_pretrained(\"tugstugi/wav2vec2-large-xlsr-53-kalmyk\").to(\"cuda\")\n",
        "  processor = Wav2Vec2Processor.from_pretrained(\"tugstugi/wav2vec2-large-xlsr-53-kalmyk\")\n",
        "\n",
        "import torch\n",
        "import librosa\n",
        "\n",
        "# imports for uploading/recording\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import Audio, display, clear_output\n",
        "from dl_colab_notebooks.audio import record_audio, upload_audio\n",
        "import numpy as np\n",
        "from scipy.io import wavfile\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "# wav to text method\n",
        "def wav_to_text(f='test.wav'):\n",
        "  audio, _ = librosa.load('test.wav', sr=SAMPLE_RATE)\n",
        "  input_dict = processor(torch.tensor(audio), return_tensors=\"pt\", padding=True)\n",
        "  logits = model(input_dict.input_values.to(\"cuda\")).logits\n",
        "  pred_ids = torch.argmax(logits, dim=-1)[0]\n",
        "  return processor.decode(pred_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05aVmrwIBUMG"
      },
      "source": [
        "## Transcribe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuZz9-Nv21SJ",
        "cellView": "form"
      },
      "source": [
        "#@markdown * Either record audio from microphone or upload audio from file (.mp3 or .wav) { run: \"auto\" }\n",
        "\n",
        "record_or_upload = \"Record\" #@param [\"Record\", \"Upload (.mp3 or .wav)\"]\n",
        "record_seconds =   10#@param {type:\"number\", min:1, max:10, step:1}\n",
        "\n",
        "def _recognize(audio):\n",
        "  display(Audio(audio, rate=SAMPLE_RATE, autoplay=True))\n",
        "  !rm -f test.wav\n",
        "  wavfile.write('test.wav', rate=SAMPLE_RATE, data=(32767*audio).astype(np.int16))\n",
        "\n",
        "  transcription = wav_to_text()\n",
        "  print('\\n\\nTRANSCRIPTION:\\n')\n",
        "  print(transcription)\n",
        "\n",
        "\n",
        "def _record_audio(b):\n",
        "  clear_output()\n",
        "  audio = record_audio(record_seconds, sample_rate=SAMPLE_RATE)\n",
        "  _recognize(audio)\n",
        "def _upload_audio(b):\n",
        "  clear_output()\n",
        "  audio = upload_audio(sample_rate=SAMPLE_RATE)\n",
        "  _recognize(audio)\n",
        "\n",
        "if record_or_upload == \"Record\":\n",
        "  button = widgets.Button(description=\"Record Speech\")\n",
        "  button.on_click(_record_audio)\n",
        "  display(button)\n",
        "else:\n",
        "  _upload_audio(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}